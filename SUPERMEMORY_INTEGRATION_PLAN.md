# Supermemory Integration Plan for OpenPoke
## Vision: Intelligent Memory, Zero Compromise on Speed

**Last Updated:** 2025-11-20
**Status:** Implementation dsReady
 
---

## Executive Summary

OpenPoke will integrate Supermemory's Memory Router to provide:
- **Unlimited conversation context** beyond token limits
- **Cross-session memory** - remember user details weeks later
- **Learning patterns** - undserstand user's email habits and preferences over time

**Key Design Decision: Selective Memory Router**
- **Only Interaction Agent** gets memory (user conversations)
- **Direct routing** for Execution Agents, Classifiers, Summarizer (stays fast)
- **Single-user model** (simplified architecture)
- **~100-200ms added latency** only where memory matters

----

## Architecture Overview

### Current Flow
```
Application ‚Üí OpenRouter Client ‚Üí MegaLLM
```

### New Flow (Conditional Routing)
```
Application ‚Üí OpenRouter Client (Smart Router) ‚Üí {
    IF memory_enabled:
        Supermemory Router ‚Üí MegaLLM
    ELSE:
        MegaLLM (direct, fast)
}
```

### Memory Scope Matrix

| Component | Memory Enabled? | Rationale | Latency Impact |
|-----------|----------------|-----------|----------------|
| **Interaction Agent** | ‚úÖ Yes | User conversations benefit most from memory | +100-200ms |
| **Execution Agent** | ‚ùå No | Already has file-based history; speed critical | None |
| **Email Search** | ‚ùå No | One-shot operations; speed critical | None |
| **Summarizer** | ‚ùå No | Stateless summarization; no memory needed | None |
| **Email Classifier** | ‚ùå No | Pattern matching; fast classification critical | None |

---

## Implementation Strategy

### Phase 1: Core Integration (Minimal Risk)
**Goal:** Add Supermemory support to `openrouter_client` without breaking existing functionality.

#### 1.1 Environment Configuration
**File:** `.env.example` and `server/config.py`

**New Environment Variables:**
```bash
# Supermemory Configuration (Optional)
SUPERMEMORY_API_KEY=your_supermemory_api_key_here
SUPERMEMORY_ENABLED=true
SUPERMEMORY_USER_ID=default_user  # Single user model

# Per-Agent Memory Control (Advanced)
SUPERMEMORY_ENABLE_INTERACTION_AGENT=true
SUPERMEMORY_ENABLE_EXECUTION_AGENT=false
SUPERMEMORY_ENABLE_SUMMARIZER=false
SUPERMEMORY_ENABLE_CLASSIFIER=false
```

**Config Changes:**
- Add `supermemory_api_key: Optional[str]` to Settings
- Add `supermemory_enabled: bool` (default False, backward compatible)
- Add `supermemory_user_id: str` (default "default_user")
- Add per-agent memory control flags

#### 1.2 Smart Router Implementation
**File:** `server/openrouter_client/client.py`

**Key Changes:**
```python
def _build_base_url(
    *,
    base_url: str,
    memory_enabled: bool,
    supermemory_api_key: Optional[str] = None,
) -> str:
    """
    Conditionally route through Supermemory Router.

    If memory_enabled=True and supermemory_api_key exists:
        Prepend Supermemory Router URL
    Else:
        Direct to provider (fast path)
    """
    if not memory_enabled or not supermemory_api_key:
        return base_url

    # Route through Supermemory
    return f"https://api.supermemory.ai/v3/{base_url}"


def _build_headers(
    *,
    api_key: str,
    memory_enabled: bool,
    supermemory_api_key: Optional[str] = None,
    user_id: Optional[str] = None,
    conversation_id: Optional[str] = None,
) -> Dict[str, str]:
    """
    Add Supermemory headers when memory is enabled.
    """
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }

    if memory_enabled and supermemory_api_key:
        headers["x-supermemory-api-key"] = supermemory_api_key

        if user_id:
            headers["x-sm-user-id"] = user_id

        if conversation_id:
            headers["x-sm-conversation-id"] = conversation_id

    return headers


async def request_chat_completion(
    *,
    model: str,
    messages: List[Dict[str, str]],
    system: Optional[str] = None,
    api_key: Optional[str] = None,
    tools: Optional[List[Dict[str, Any]]] = None,
    base_url: str = AnannasBaseURL,
    prompt_cache_key: Optional[str] = None,
    memory_enabled: bool = False,  # NEW: explicit memory control
    conversation_id: Optional[str] = None,  # NEW: conversation tracking
) -> Dict[str, Any]:
    """
    Request chat completion with optional memory routing.

    New Parameters:
        memory_enabled: If True, route through Supermemory
        conversation_id: Optional conversation ID for memory tracking
    """
    settings = get_settings()

    # Build URL (conditional routing)
    final_base_url = _build_base_url(
        base_url=base_url,
        memory_enabled=memory_enabled,
        supermemory_api_key=settings.supermemory_api_key,
    )

    # Build headers (add Supermemory auth if needed)
    headers = _build_headers(
        api_key=api_key or settings.anannas_api_key,
        memory_enabled=memory_enabled,
        supermemory_api_key=settings.supermemory_api_key,
        user_id=settings.supermemory_user_id,
        conversation_id=conversation_id,
    )

    # Rest of function stays the same...
    # Build payload, make request, handle errors
```

#### 1.3 Interaction Agent Integration
**File:** `server/agents/interaction_agent/runtime.py`

**Changes:**
```python
async def _make_llm_call(
    self,
    system_prompt: str,
    messages: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """Make an LLM call via Ananas (with optional memory)."""

    # Determine conversation ID for memory tracking
    # Use chat log length or generate from timestamp
    conversation_id = self._get_or_create_conversation_id()

    logger.debug(
        "Interaction agent calling LLM",
        extra={
            "model": self.model,
            "tools": len(self.tool_schemas),
            "memory_enabled": self.settings.supermemory_enabled,
        },
    )

    return await request_chat_completion(
        model=self.model,
        messages=messages,
        system=system_prompt,
        api_key=self.api_key,
        tools=self.tool_schemas,
        prompt_cache_key="interaction_agent_v1",
        memory_enabled=self.settings.supermemory_enabled,  # NEW
        conversation_id=conversation_id,  # NEW
    )


def _get_or_create_conversation_id(self) -> str:
    """
    Generate stable conversation ID for memory tracking.

    Options:
    1. Use existing chat session ID (if available)
    2. Generate from date (one conversation per day)
    3. Use fixed ID for single-user (simplest)
    """
    # Option 3: Single user, single conversation
    # This means all conversations share memory
    return f"openpoke_{self.settings.supermemory_user_id}"

    # Option 2: Daily conversations
    # from datetime import datetime
    # return f"openpoke_{datetime.now().strftime('%Y%m%d')}"
```

**Note:** Other agents (Execution, Summarizer, Classifier) keep their existing `_make_llm_call` without memory parameters (direct routing, fast).

---

### Phase 2: Enhanced Features (Optional)

#### 2.1 Conversation Management
- Add conversation reset endpoint
- Allow user to start "new conversation" (new conversation ID)
- View memory stats (chunks stored, retrieved)

#### 2.2 Memory Diagnostics
- Log Supermemory response headers:
  - `x-supermemory-context-modified`
  - `x-supermemory-chunks-created`
  - `x-supermemory-chunks-retrieved`
- Add metrics endpoint for memory usage

#### 2.3 Advanced Memory Control
- Per-conversation memory on/off toggle
- Manual memory injection (via Simple API for critical info)
- Memory export/import for backup

---

## Token Optimization Strategy

### Current Token Usage (Without Memory)
**Interaction Agent:**
- System prompt: ~500 tokens
- Conversation history: 0-10,000+ tokens (grows over time)
- User message: ~50-500 tokens
- **Total:** 550-11,000+ tokens per call

**Problem:** Long conversations hit token limits, require summarization (costs extra LLM call)

### With Supermemory Memory Router
**Interaction Agent:**
- System prompt: ~500 tokens
- Supermemory auto-chunked context: ~1,000-2,000 tokens (relevant only)
- User message: ~50-500 tokens
- **Total:** 1,550-3,000 tokens per call (capped)

**Benefits:**
- ‚úÖ 70% reduction in tokens for long conversations
- ‚úÖ No need for summarization LLM calls (save cost + latency)
- ‚úÖ Unlimited conversation length
- ‚úÖ Better context quality (semantic search vs. truncation)

### Cost Analysis
**Current System:**
- Long conversation: 10k tokens √ó $0.003/1k = $0.03 per call
- Summarization: 10k tokens + 2k output = $0.036 per summary
- **Total for 100 calls with 10 summaries:** $3.36

**With Supermemory:**
- Memory Router call: 2k tokens √ó $0.003/1k = $0.006 per call
- Supermemory storage: $1 per million tokens (negligible)
- **Total for 100 calls:** $0.60 + $0.01 storage = $0.61

**Savings:** 82% reduction in LLM costs for long conversations

---

## Latency Optimization

### Baseline Performance
| Operation | Current Latency | With Memory Router | Delta |
|-----------|----------------|-------------------|--------|
| Interaction Agent (short) | 1,000ms | 1,100-1,200ms | +100-200ms |
| Interaction Agent (long) | 2,000ms | 1,500-1,700ms | -300 to -500ms* |
| Execution Agent | 800ms | 800ms (unchanged) | 0ms |
| Email Classifier | 500ms | 500ms (unchanged) | 0ms |

*Long conversations are faster with memory because Supermemory reduces token count, speeding up LLM processing.

### Optimization Techniques

#### 1. Async Memory Creation
Supermemory creates memories asynchronously - doesn't block response.

#### 2. Smart Conversation ID Caching
```python
# Cache conversation ID per session (avoid regeneration)
@lru_cache(maxsize=1)
def _get_conversation_id(self) -> str:
    return f"openpoke_{self.settings.supermemory_user_id}"
```

#### 3. Selective Memory Retrieval
Only Interaction Agent uses memory - all other paths stay fast.

#### 4. HTTP/2 Connection Pooling
Use `httpx.AsyncClient` persistent connections for both Supermemory and Ananas.

---

## User Experience Improvements

### Before Supermemory
**User:** "What's my email?"
**Bot:** "I don't see that in our current conversation."
*(User told the bot their email 2 days ago)*

**User:** *Long conversation (100+ messages)*
**Bot:** "Let me summarize our conversation..." *(5-10 second delay)*
**User:** *Loses context, conversation feels choppy*

### After Supermemory
**User:** "What's my email?"
**Bot:** "Your email is alice@example.com." ‚ú®
*(Recalls from 2 days ago)*

**User:** *Long conversation (100+ messages)*
**Bot:** *Instant responses, never needs to summarize*
**User:** *Seamless experience, bot remembers everything*

### Additional UX Benefits
- **Proactive suggestions:** "You usually check your work email around this time. Should I fetch today's important messages?"
- **Context awareness:** "I see you're working on the Q4 report. Need me to find related emails?"
- **Personalization:** "Based on past conversations, you prefer summaries over full emails. Should I continue that way?"

---

## Testing Strategy

### Phase 1: Development Testing

#### Unit Tests
```python
# test_openrouter_client.py

async def test_memory_routing_enabled():
    """Test that memory-enabled calls route through Supermemory"""
    url = _build_base_url(
        base_url="https://api.anannas.ai/v1",
        memory_enabled=True,
        supermemory_api_key="test_key"
    )
    assert url == "https://api.supermemory.ai/v3/https://api.anannas.ai/v1"


async def test_memory_routing_disabled():
    """Test that memory-disabled calls route direct"""
    url = _build_base_url(
        base_url="https://api.anannas.ai/v1",
        memory_enabled=False,
        supermemory_api_key="test_key"
    )
    assert url == "https://api.anannas.ai/v1"


async def test_headers_with_memory():
    """Test Supermemory headers are added correctly"""
    headers = _build_headers(
        api_key="anannas_key",
        memory_enabled=True,
        supermemory_api_key="sm_key",
        user_id="user123",
        conversation_id="conv456"
    )
    assert headers["x-supermemory-api-key"] == "sm_key"
    assert headers["x-sm-user-id"] == "user123"
    assert headers["x-sm-conversation-id"] == "conv456"
```

#### Integration Tests
1. **Backward Compatibility:** Existing flows work without Supermemory env vars
2. **Memory Persistence:** Chat, exit, restart, verify memory recall
3. **Fallback Handling:** Supermemory down ‚Üí requests pass through
4. **Header Validation:** Response headers contain Supermemory diagnostics

### Phase 2: Manual Testing

#### Test Case 1: Cross-Session Memory
```
Session 1:
User: "My name is Alice and I work at Acme Corp"
Bot: "Nice to meet you, Alice! How can I help you at Acme Corp today?"
[Exit app]

Session 2 (next day):
User: "What's my name?"
Bot: "Your name is Alice, and you work at Acme Corp."
‚úÖ PASS: Memory persisted across sessions
```

#### Test Case 2: Long Conversation
```
User: [Send 50 messages about various topics]
Bot: [Responses stay fast, no summarization delays]
User: "What did we discuss earlier about email filters?"
Bot: "Earlier, you asked about setting up filters for..."
‚úÖ PASS: Handles 50+ messages without token limit issues
```

#### Test Case 3: Performance (Execution Agents)
```
User: "Search my email for invoices"
[Execution agent runs]
Measure latency: Should be same as before (~800ms)
‚úÖ PASS: Execution agents not affected by memory routing
```

### Phase 3: Production Monitoring

#### Metrics to Track
- **Latency P50/P95/P99:** Interaction Agent calls
- **Token usage:** Compare before/after memory
- **Memory chunks:** Created vs. retrieved ratio
- **Error rate:** Supermemory failures (should auto-fallback)
- **Cost:** Supermemory API usage vs. token savings

#### Logging Strategy
```python
logger.info(
    "LLM call completed",
    extra={
        "agent": "interaction",
        "memory_enabled": True,
        "latency_ms": 1150,
        "tokens_processed": response.usage.total_tokens,
        "memory_chunks_created": response.headers.get("x-supermemory-chunks-created"),
        "memory_chunks_retrieved": response.headers.get("x-supermemory-chunks-retrieved"),
    }
)
```

---

## Rollout Plan

### Step 1: Add Environment Variables (Non-Breaking)
- Update `.env.example`
- Update `server/config.py`
- Deploy: No functional change (memory disabled by default)

### Step 2: Implement Smart Router (Backward Compatible)
- Add `memory_enabled` parameter to `request_chat_completion` (default False)
- Add URL and header building logic
- Deploy: No functional change (memory still disabled)

### Step 3: Enable for Interaction Agent (Staged Rollout)
- Update Interaction Agent runtime
- Set `SUPERMEMORY_ENABLED=true` in `.env`
- Add Supermemory API key
- Deploy: Memory now active for user conversations only

### Step 4: Monitor and Optimize
- Watch latency metrics
- Check memory chunk creation/retrieval
- Adjust conversation ID strategy if needed
- Fine-tune which agents use memory (if desired)

---

## Risk Mitigation

### Risk 1: Supermemory API Downtime
**Impact:** High (breaks Interaction Agent)
**Mitigation:**
- Supermemory Router has built-in fallback (passes through on error)
- Add timeout handling (5-second max wait)
- Add circuit breaker pattern (after 3 failures, disable memory for 5 minutes)

```python
from datetime import datetime, timedelta

class MemoryCircuitBreaker:
    def __init__(self):
        self.failure_count = 0
        self.disabled_until: Optional[datetime] = None

    def should_use_memory(self) -> bool:
        if self.disabled_until and datetime.now() < self.disabled_until:
            return False  # Circuit open
        return True

    def record_failure(self):
        self.failure_count += 1
        if self.failure_count >= 3:
            self.disabled_until = datetime.now() + timedelta(minutes=5)
            logger.warning("Memory circuit breaker opened")

    def record_success(self):
        self.failure_count = 0
        self.disabled_until = None
```

### Risk 2: Unexpected Latency Increase
**Impact:** Medium (user-facing slowness)
**Mitigation:**
- Monitor P95 latency
- Alert if >300ms increase
- Quick rollback: set `SUPERMEMORY_ENABLED=false`

### Risk 3: Memory Quality Issues
**Impact:** Low (wrong memories retrieved)
**Mitigation:**
- Supermemory's semantic search is production-tested
- Can clear memories via API if needed
- Conversation ID scoping prevents memory leakage

### Risk 4: Cost Overruns
**Impact:** Low (predictable costs)
**Mitigation:**
- Free tier: 100k tokens (covers ~33 long conversations)
- After free tier: $20/month flat + $1 per million tokens
- Monitor usage via Supermemory dashboard
- Set budget alerts at $50/month

---

## Code Style & Best Practices

### Design Principles
1. **Backward Compatibility:** Works without Supermemory env vars
2. **Explicit Over Implicit:** `memory_enabled` parameter is explicit
3. **Fail Safe:** Memory failures don't break core functionality
4. **Single Responsibility:** Memory routing logic isolated in `openrouter_client`
5. **Configuration Driven:** All behavior controlled by env vars

### Code Patterns

#### ‚úÖ DO: Explicit Memory Control
```python
# Clear intent, easy to test
response = await request_chat_completion(
    model=model,
    messages=messages,
    memory_enabled=True,  # Explicit
)
```

#### ‚ùå DON'T: Implicit Memory Detection
```python
# Bad: magic behavior, hard to debug
response = await request_chat_completion(model=model, messages=messages)
# Secretly checks env vars and routes through memory
```

#### ‚úÖ DO: Graceful Fallback
```python
try:
    response = await make_memory_call()
except MemoryError:
    logger.warning("Memory unavailable, using direct routing")
    response = await make_direct_call()
```

#### ‚ùå DON'T: Hard Failure
```python
response = await make_memory_call()  # Crashes if Supermemory down
```

#### ‚úÖ DO: Structured Logging
```python
logger.info(
    "Memory routing active",
    extra={
        "conversation_id": conv_id,
        "user_id": user_id,
        "chunks_retrieved": 5,
    }
)
```

#### ‚ùå DON'T: String Logging
```python
logger.info(f"Memory active for {conv_id} with {user_id}")  # Harder to parse
```

---

## Alternative Approaches Considered

### Alternative 1: Simple Memory API (Manual Control)
**Pros:**
- Full control over when memories are created/retrieved
- Lower latency (only pay when explicitly needed)
- Better for selective memory (e.g., only store email patterns)

**Cons:**
- More code to write and maintain
- Manual memory management (must decide what to store/retrieve)
- Risk of forgetting to store important context

**Decision:** ‚ùå Rejected. You specified "only memory router" for simplicity.

### Alternative 2: Memory Router for All Agents
**Pros:**
- Uniform behavior across all agents
- Simpler configuration (one setting for all)

**Cons:**
- Unnecessary latency for stateless operations (classifier, summarizer)
- Higher cost (more tokens processed through Supermemory)
- Execution agents already have file-based history

**Decision:** ‚ùå Rejected. Optimizing for speed means selective memory.

### Alternative 3: Hybrid (Router + API)
**Pros:**
- Best of both worlds
- Router for conversations, API for explicit memory (e.g., user preferences)

**Cons:**
- More complexity
- Two integration points to maintain

**Decision:** üîÑ Future Enhancement. Start with Router only, add API later if needed.

---

## Future Enhancements (Post-MVP)

### 1. Multi-User Support
When scaling beyond single user:
- Add user authentication/identification
- Use `x-sm-user-id` from auth token
- Implement user-scoped conversation IDs
- Add memory isolation tests

### 2. Memory Management UI
- View stored memories (via Simple API)
- Delete specific memories
- Export memory archive
- Memory usage dashboard

### 3. Advanced Memory Features
- **Manual Memory Injection:** Store critical info immediately (e.g., "Remember: user is left-handed")
- **Memory Tagging:** Organize memories by topic (work, personal, emails)
- **Memory Search:** Search across memories (separate from chat)
- **Memory Analytics:** What does the bot know about me?

### 4. Performance Optimizations
- HTTP/2 connection pooling for Supermemory
- Response streaming with memory (if supported)
- Predictive memory preloading based on context

### 5. Email-Specific Memory
- Store frequently contacted emails
- Remember email preferences (summary vs full)
- Learn importance patterns (not just classifier, but memory-based)

---

## Success Metrics

### Technical Metrics
- ‚úÖ **Latency:** P95 < 1.5s for Interaction Agent (from ~2s+ in long conversations)
- ‚úÖ **Token Reduction:** 50-70% fewer tokens in conversations >50 messages
- ‚úÖ **Uptime:** 99.9% (with fallback handling)
- ‚úÖ **Memory Recall:** >90% accuracy on cross-session queries

### User Experience Metrics
- ‚úÖ **User Satisfaction:** Qualitative feedback ("bot remembers me!")
- ‚úÖ **Conversation Length:** Average messages per session increases
- ‚úÖ **Repeat Usage:** Daily active usage increases
- ‚úÖ **Error Rate:** No increase in user-reported errors

### Cost Metrics
- ‚úÖ **ROI:** Token cost savings > Supermemory cost (for long conversations)
- ‚úÖ **Budget:** Stay within $50/month for single user

---

## Dependencies & Prerequisites

### Required Services
1. **Supermemory API Key**
   - Sign up: https://console.supermemory.ai
   - Free tier: 100k tokens
   - Paid: $20/month after free tier

2. **Existing Services (No Changes)**
   - Ananas AI API key (already have)
   - Composio API key (already have)

### Python Dependencies
No new dependencies needed! Supermemory Memory Router works with existing `httpx` client.

### Environment Setup
```bash
# Add to .env
SUPERMEMORY_API_KEY=sm_...
SUPERMEMORY_ENABLED=true
SUPERMEMORY_USER_ID=default_user
```

---

## Documentation Updates Needed

### 1. README.md
Add Supermemory setup section:
```markdown
### Memory Configuration (Optional)

OpenPoke can use Supermemory to give your assistant unlimited memory:

1. Sign up at https://console.supermemory.ai
2. Get your API key
3. Add to `.env`:
   ```
   SUPERMEMORY_API_KEY=your_key_here
   SUPERMEMORY_ENABLED=true
   ```
4. Restart the server

Your assistant will now remember conversations across sessions!
```

### 2. CLAUDE.md
Update architecture section:
```markdown
### Memory Integration (Supermemory)

Interaction Agent can optionally use Supermemory for cross-session memory:
- When enabled, LLM calls route through Supermemory Router
- Automatically manages context chunking and retrieval
- Adds ~100-200ms latency but provides unlimited context
- Other agents bypass memory for maximum speed
```

### 3. API Documentation
Add memory endpoints (future):
- GET `/api/memory/stats` - Memory usage statistics
- POST `/api/memory/reset` - Clear conversation memory
- GET `/api/memory/conversations` - List conversations

---

## Appendix A: Supermemory Router Specification

### Request Flow
```
1. Application makes request to Supermemory Router
2. Router checks for existing memories (x-sm-user-id + x-sm-conversation-id)
3. Router retrieves relevant memory chunks
4. Router modifies request:
   - Removes old/irrelevant messages
   - Appends memory context to system prompt
5. Router forwards optimized request to Ananas AI
6. Ananas returns response
7. Router extracts new memories asynchronously
8. Router returns response to application (with diagnostic headers)
```

### Headers Sent
| Header | Purpose | Example |
|--------|---------|---------|
| `Authorization` | Ananas API key | `Bearer anannas_key_...` |
| `x-supermemory-api-key` | Supermemory auth | `sm_key_...` |
| `x-sm-user-id` | User identification | `default_user` |
| `x-sm-conversation-id` | Conversation scoping | `openpoke_default_user` |

### Headers Received
| Header | Meaning | Usage |
|--------|---------|-------|
| `x-supermemory-conversation-id` | Conversation ID | Log for debugging |
| `x-supermemory-context-modified` | Whether context was changed | Monitor effectiveness |
| `x-supermemory-tokens-processed` | Tokens processed | Cost tracking |
| `x-supermemory-chunks-created` | New memory chunks | Memory growth |
| `x-supermemory-chunks-retrieved` | Chunks added to context | Memory usage |

### Error Handling
- **Supermemory down:** Request passes through to Ananas (transparent fallback)
- **Invalid API key:** Error returned, caught by circuit breaker
- **Timeout:** Request fails, falls back to direct routing

---

## Appendix B: File Changes Checklist

### Files to Modify
- [ ] `.env.example` - Add Supermemory env vars
- [ ] `server/config.py` - Add Supermemory settings
- [ ] `server/openrouter_client/client.py` - Smart routing logic
- [ ] `server/agents/interaction_agent/runtime.py` - Enable memory calls
- [ ] `README.md` - Add setup documentation
- [ ] `CLAUDE.md` - Update architecture section

### Files to Create
- [ ] `server/openrouter_client/memory_circuit_breaker.py` - Circuit breaker (optional)
- [ ] `tests/test_memory_routing.py` - Unit tests
- [ ] `tests/test_memory_integration.py` - Integration tests

### Files Unchanged (Intentionally)
- ‚úÖ `server/agents/execution_agent/runtime.py` - Direct routing, no memory
- ‚úÖ `server/services/conversation/summarization/` - No memory needed
- ‚úÖ `server/services/gmail/importance_classifier.py` - No memory needed

---

## Appendix C: Configuration Reference

### Complete .env Example
```bash
# === Required: Core Services ===
ANANNAS_API_KEY=anannas_key_here
COMPOSIO_API_KEY=composio_key_here
COMPOSIO_GMAIL_AUTH_CONFIG_ID=config_id_here

# === Optional: Supermemory Memory ===
SUPERMEMORY_API_KEY=sm_key_here
SUPERMEMORY_ENABLED=true
SUPERMEMORY_USER_ID=default_user

# === Optional: Advanced Memory Control ===
SUPERMEMORY_ENABLE_INTERACTION_AGENT=true
SUPERMEMORY_ENABLE_EXECUTION_AGENT=false
SUPERMEMORY_ENABLE_SUMMARIZER=false
SUPERMEMORY_ENABLE_CLASSIFIER=false

# === Optional: Models ===
INTERACTION_AGENT_MODEL=grok-4-1-fast-reasoning-latest
EXECUTION_AGENT_MODEL=grok-4-1-fast-reasoning-latest
EXECUTION_AGENT_SEARCH_MODEL=grok-4-1-fast-reasoning-latest
SUMMARIZER_MODEL=grok-4-1-fast-reasoning-latest
EMAIL_CLASSIFIER_MODEL=grok-4-1-fast-reasoning-latest

# === Optional: Server ===
OPENPOKE_HOST=0.0.0.0
OPENPOKE_PORT=8001
OPENPOKE_CORS_ALLOW_ORIGINS=*
```

### Settings Priority
1. Environment variables (highest priority)
2. `.env` file
3. Default values in `config.py` (lowest priority)

---

## Questions & Answers

### Q: Will this break my existing setup?
**A:** No! Memory is disabled by default. Without Supermemory env vars, everything works exactly as before.

### Q: What happens if Supermemory goes down?
**A:** Memory Router has automatic fallback - requests pass directly to Ananas. Your app stays online.

### Q: Can I turn off memory temporarily?
**A:** Yes! Set `SUPERMEMORY_ENABLED=false` and restart. No code changes needed.

### Q: How much will this cost?
**A:** Free tier covers 100k tokens (~33 long conversations). After that, $20/month + $1 per million tokens. For a single user, likely <$25/month total.

### Q: Will responses be slower?
**A:** Short conversations: +100-200ms. Long conversations: actually *faster* (fewer tokens to process). Execution agents stay fast (no memory).

### Q: Can I add multiple users later?
**A:** Yes! Change `supermemory_user_id` to come from user auth token instead of config. Memory automatically isolates per user.

### Q: Can I see what's stored in memory?
**A:** Not in MVP. Future enhancement: memory viewer using Simple API.

### Q: What if I want even more control?
**A:** Future enhancement: add Simple Memory API alongside Router for manual memory management.

---

## Conclusion

This implementation plan provides:
‚úÖ **Elegant Architecture** - Smart routing, no bloat
‚úÖ **Speed Optimized** - Only Interaction Agent uses memory
‚úÖ **Production Ready** - Error handling, fallbacks, monitoring
‚úÖ **User Focused** - Better conversations, cross-session memory
‚úÖ **Cost Efficient** - 70% token reduction in long conversations
‚úÖ **Future Proof** - Easy to extend with multi-user, Simple API, etc.

**Estimated Implementation Time:** 4-6 hours for MVP, 2-3 hours for testing.

**Ready to implement?** Let's start with Phase 1: Core Integration.

---

*This plan embodies the "Think Different" philosophy - questioning every assumption, obsessing over details, and crafting a solution so elegant it feels inevitable.*
